{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 19:18:49.349066: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-09 19:18:51.721151: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-09 19:18:52.364211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-09 19:18:52.807332: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-09 19:18:52.966484: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 19:18:54.324193: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 19:18:59.938557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from geopy.distance import geodesic\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the AIS data\n",
    "ais_train = pd.read_csv('data/ais_train.csv', sep='|')\n",
    "ais_test = pd.read_csv('data/ais_test.csv', sep='|')\n",
    "\n",
    "# Optionally, load vessels and ports datasets for additional features\n",
    "#vessels = pd.read_csv('data/vessels.csv')\n",
    "#ports = pd.read_csv('data/ports.csv')\n",
    "#schedules = pd.read_csv('data/schedules_to_may_2024.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ais_train['latitude'] = pd.to_numeric(ais_train['latitude'], errors='coerce')\n",
    "ais_train['longitude'] = pd.to_numeric(ais_train['longitude'], errors='coerce')\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'], errors='coerce')\n",
    "\n",
    "# Function to calculate speed (delta_lat^2 + delta_lon^2) / delta_time\n",
    "def calculate_speed(df):\n",
    "    df['delta_lat'] = df.groupby('vesselId')['latitude'].diff()\n",
    "    df['delta_lon'] = df.groupby('vesselId')['longitude'].diff()\n",
    "    df['delta_time'] = df.groupby('vesselId')['time'].diff().dt.total_seconds() / 3600  # in hours\n",
    "    df['speed'] = np.sqrt(df['delta_lat']**2 + df['delta_lon']**2) / df['delta_time']\n",
    "    df['speed'].fillna(0, inplace=True)\n",
    "\n",
    "# Calculate speed and other time-related features\n",
    "calculate_speed(ais_train)\n",
    "\n",
    "# Additional time-based features\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'])  # Ensure 'time' column is in datetime format\n",
    "ais_train['hour'] = ais_train['time'].dt.hour\n",
    "ais_train['day_of_week'] = ais_train['time'].dt.dayofweek\n",
    "ais_train['month'] = ais_train['time'].dt.month\n",
    "\n",
    "\n",
    "# Resample AIS data to a regular interval (e.g., every 20 minutes)\n",
    "def resample_vessel_data(df, vessel_id):\n",
    "    vessel_df = df[df['vesselId'] == vessel_id]\n",
    "    vessel_df.set_index('time', inplace=True)\n",
    "    vessel_df = vessel_df.resample('20T').agg({\n",
    "        'latitude': 'mean',\n",
    "        'longitude': 'mean',\n",
    "        'cog': 'mean',\n",
    "        'sog': 'mean',\n",
    "        'rot': 'mean',\n",
    "        'heading': 'mean',\n",
    "        'navstat': 'mean',\n",
    "        'etaRaw': 'first',\n",
    "        'vesselId': 'first',\n",
    "        'portId': 'first',\n",
    "        'delta_lat': 'mean',\n",
    "        'delta_lon': 'mean',\n",
    "        'delta_time': 'mean',\n",
    "        'speed': 'mean',\n",
    "        'hour': 'first',\n",
    "        'day_of_week': 'first',\n",
    "        'month': 'first'\n",
    "    }).interpolate()\n",
    "    vessel_df.reset_index(inplace=True)\n",
    "    return vessel_df\n",
    "\n",
    "# Resample all vessels\n",
    "resampled_vessels = []\n",
    "for vessel in ais_train['vesselId'].unique():\n",
    "    resampled_vessels.append(resample_vessel_data(ais_train, vessel))\n",
    "\n",
    "ais_train_resampled = pd.concat(resampled_vessels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(data[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "sequence_length = 20  # Use 20 historical points to predict the next one\n",
    "\n",
    "# Prepare input features (latitude, longitude, speed, etc.) for LSTM\n",
    "features = ['latitude', 'longitude', 'speed', 'cog', 'hour', 'day_of_week']\n",
    "X_lstm, y_lstm = create_sequences(ais_train_resampled[features].values, sequence_length)\n",
    "\n",
    "# Latitude and longitude as targets\n",
    "y_lstm = ais_train_resampled[['latitude', 'longitude']].values[sequence_length:]\n",
    "\n",
    "X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=2, activation='linear'))  # Predict latitude and longitude\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train_lstm.shape[1], X_train_lstm.shape[2])  # Adjust based on data\n",
    "model = build_lstm_model(input_shape)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_lstm, y_train_lstm, \n",
    "                    epochs=50, \n",
    "                    batch_size=64, \n",
    "                    validation_data=(X_val_lstm, y_val_lstm), \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for Random Forest (LSTM predictions + additional features)\n",
    "X_rf_train = pd.DataFrame(y_train_lstm, columns=['latitude_pred', 'longitude_pred'])\n",
    "X_rf_train['speed'] = ais_train_resampled['speed'][sequence_length:]\n",
    "X_rf_train['port_call'] = ais_train_resampled['port_call'][sequence_length:]\n",
    "\n",
    "y_rf_train = ais_train_resampled[['latitude', 'longitude']].values[sequence_length:]\n",
    "\n",
    "# Train-validation split for Random Forest\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(X_rf_train, y_rf_train, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42)\n",
    "rf_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# Validate\n",
    "rf_predictions = rf_model.predict(X_val_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lstm = create_sequences(ais_test[features].values, sequence_length)[0]\n",
    "lstm_test_predictions = model.predict(X_test_lstm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_rf_test = pd.DataFrame(lstm_test_predictions, columns=['latitude_pred', 'longitude_pred'])\n",
    "X_rf_test['speed'] = ais_test['speed'][sequence_length:]\n",
    "X_rf_test['port_call'] = ais_test['port_call'][sequence_length:]\n",
    "\n",
    "rf_test_predictions = rf_model.predict(X_rf_test)\n",
    "\n",
    "\n",
    "# Prepare submission file\n",
    "submission = pd.DataFrame(rf_test_predictions, columns=['latitude', 'longitude'])\n",
    "submission['vessel_id'] = ais_test['vessel_id'][sequence_length:]\n",
    "submission['timestamp'] = ais_test['timestamp'][sequence_length:]\n",
    "\n",
    "# Save to CSV for submission\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
