{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db98cfe0-7a6b-45dd-bdbe-da9010389e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Load the AIS training data\n",
    "ais_train = pd.read_csv('data/ais_train.csv', delimiter='|')\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Haversine formula to calculate distance between two latitude-longitude points\n",
    "    R = 6371  # Radius of Earth in kilometers\n",
    "    dlat = np.radians(lat2 - lat1)\n",
    "    dlon = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "    # Calculate the bearing angle between two lat-long points\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2 - lon1\n",
    "    x = np.sin(dlon) * np.cos(lat2)\n",
    "    y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n",
    "    return np.degrees(np.arctan2(x, y))\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_ais_data(df):\n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Sort by vesselId and time\n",
    "    df = df.sort_values(by=['vesselId', 'time'])\n",
    "    \n",
    "    df = df.dropna()\n",
    "    # Handle NaN values with Linear Regression Imputation\n",
    "    #df = linear_regression_impute(df)\n",
    "    \n",
    "    # Feature extraction: Time differences, velocity, and direction (heading)\n",
    "    df['time_diff'] = df.groupby('vesselId')['time'].diff().dt.total_seconds() / 3600.0  # in hours\n",
    "    df['lat_diff'] = df.groupby('vesselId')['latitude'].diff().fillna(0)  # Fill NaNs with 0\n",
    "    df['lon_diff'] = df.groupby('vesselId')['longitude'].diff().fillna(0)  # Fill NaNs with 0\n",
    "    df['distance'] = df.apply(lambda row: geodesic((row['latitude'], row['longitude']),\n",
    "                                                   (row['latitude'] - row['lat_diff'],\n",
    "                                                   row['longitude'] - row['lon_diff'])).km, axis=1)\n",
    "    \n",
    "    df['haversine_distance'] = df.apply(lambda row: haversine(row['latitude'], row['longitude'],\n",
    "                                                              row['latitude'] - row['lat_diff'],\n",
    "                                                              row['longitude'] - row['lon_diff']), axis=1)\n",
    "    \n",
    "    # Calculate speed (in km/h)\n",
    "    df['speed'] = df['distance'] / df['time_diff']\n",
    "    \n",
    "    # Calculate acceleration (change in speed)\n",
    "    df['acceleration'] = df.groupby('vesselId')['speed'].diff() / df['time_diff']\n",
    "    \n",
    "    # Calculate change in heading over time\n",
    "    df['heading_diff'] = df.groupby('vesselId')['heading'].diff() / df['time_diff']\n",
    "    \n",
    "    # Calculate relative bearing (difference between course over ground and heading)\n",
    "    df['relative_bearing'] = df['cog'] - df['heading']\n",
    "    \n",
    "    # Calculate turn rate (change in course over ground over time)\n",
    "    df['turn_rate'] = df.groupby('vesselId')['cog'].diff() / df['time_diff']\n",
    "    \n",
    "    # Extract hour of the day\n",
    "    df['hour_of_day'] = df['time'].dt.hour\n",
    "    \n",
    "    # Calculate cumulative distance traveled\n",
    "    df['cumulative_distance'] = df.groupby('vesselId')['distance'].cumsum()\n",
    "    \n",
    "    # Calculate days since last port arrival (example)\n",
    "    df['days_since_last_port'] = (df['time'] - df.groupby('vesselId')['time'].transform('min')).dt.total_seconds() / (3600 * 24)    \n",
    "    \n",
    "    # Additional feature engineering\n",
    "    df['bearing_angle'] = df.apply(lambda row: calculate_bearing(row['latitude'] - row['lat_diff'], \n",
    "                                                                 row['longitude'] - row['lon_diff'], \n",
    "                                                                 row['latitude'], \n",
    "                                                                 row['longitude']), axis=1)\n",
    "    df['cumulative_speed'] = df.groupby('vesselId')['speed'].cumsum()\n",
    "    df['bearing_change_rate'] = df['bearing_angle'].diff() / df['time_diff']\n",
    "    df['cumulative_heading_change'] = df.groupby('vesselId')['heading_diff'].cumsum()\n",
    "    \n",
    "    df['previous_latitude'] = df['latitude'] - df['lat_diff']\n",
    "    df['previous_longitude'] = df['longitude'] - df['lon_diff']\n",
    "    df['distance_from_prev_point'] = df['distance']\n",
    "    df['time_gap_from_prev_point'] = df['time_diff']\n",
    "\n",
    "    df['delta_cog'] = df.groupby('vesselId')['cog'].diff()\n",
    "    df['delta_rot'] = df.groupby('vesselId')['rot'].diff()\n",
    "\n",
    "    df['time_gap_from_start'] = (df['time'] - df.groupby('vesselId')['time'].transform('min')).dt.total_seconds() / 3600.0\n",
    "    df['cumulative_time_difference'] = df.groupby('vesselId')['time_diff'].cumsum()\n",
    "\n",
    "    # Fill NaNs resulting from differencing and rolling calculations\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to impute missing values using Linear Regression\n",
    "def linear_regression_impute(df):\n",
    "    # Isolate rows with missing latitude or longitude\n",
    "    missing_lat = df['latitude'].isnull()\n",
    "    missing_lon = df['longitude'].isnull()\n",
    "\n",
    "    # Prepare the feature set for prediction\n",
    "    feature_columns = ['time_diff', 'lat_diff', 'lon_diff', 'speed']\n",
    "    \n",
    "    # Train Linear Regression for latitude\n",
    "    if missing_lat.any():\n",
    "        # Train data: Drop rows where 'latitude' is missing and also drop NaNs in the feature columns\n",
    "        lat_train = df[~missing_lat].dropna(subset=feature_columns)\n",
    "        X_lat_train = lat_train[feature_columns]\n",
    "        y_lat_train = lat_train['latitude']\n",
    "\n",
    "        # Fit the model\n",
    "        lat_model = LinearRegression()\n",
    "        lat_model.fit(X_lat_train, y_lat_train)\n",
    "\n",
    "        # Predict missing latitudes\n",
    "        X_lat_missing = df[missing_lat][feature_columns].fillna(0)\n",
    "        df.loc[missing_lat, 'latitude'] = lat_model.predict(X_lat_missing)\n",
    "\n",
    "    # Train Linear Regression for longitude\n",
    "    if missing_lon.any():\n",
    "        # Train data: Drop rows where 'longitude' is missing and also drop NaNs in the feature columns\n",
    "        lon_train = df[~missing_lon].dropna(subset=feature_columns)\n",
    "        X_lon_train = lon_train[feature_columns]\n",
    "        y_lon_train = lon_train['longitude']\n",
    "\n",
    "        # Fit the model\n",
    "        lon_model = LinearRegression()\n",
    "        lon_model.fit(X_lon_train, y_lon_train)\n",
    "\n",
    "        # Predict missing longitudes\n",
    "        X_lon_missing = df[missing_lon][feature_columns].fillna(0)\n",
    "        df.loc[missing_lon, 'longitude'] = lon_model.predict(X_lon_missing)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess the training data\n",
    "ais_train = preprocess_ais_data(ais_train)\n",
    "\n",
    "# Select relevant features for training\n",
    "#features = ['cog', 'sog', 'rot', 'heading', 'speed']\n",
    "# Final feature list\n",
    "features = ['cog', 'sog', 'rot', 'heading', 'speed', 'acceleration', 'heading_diff', \n",
    "            'relative_bearing', 'turn_rate', 'hour_of_day', 'haversine_distance', \n",
    "            'cumulative_distance', 'days_since_last_port', 'bearing_angle', 'cumulative_speed',\n",
    "            'bearing_change_rate', 'cumulative_heading_change', 'previous_latitude', \n",
    "            'previous_longitude', 'distance_from_prev_point', 'time_gap_from_prev_point', 'delta_cog', 'delta_rot', \n",
    "            'time_gap_from_start', 'cumulative_time_difference']\n",
    "\n",
    "target_lat = 'latitude'\n",
    "target_lon = 'longitude'\n",
    "\n",
    "# Use MinMaxScaler to scale the features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(ais_train[features])\n",
    "\n",
    "# Prepare the train data\n",
    "X_train = scaled_features\n",
    "y_train_lat = ais_train[target_lat].values\n",
    "y_train_lon = ais_train[target_lon].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21168ff-5c37-46e3-b773-7f02c9ff1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train a Random Forest model for latitude\n",
    "#rf_lat = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "#rf_lat.fit(X_train, y_train_lat)\n",
    "\n",
    "# Train a Random Forest model for longitude\n",
    "#rf_lon = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "#rf_lon.fit(X_train, y_train_lon)\n",
    "\n",
    "# Print model summary\n",
    "#print(\"Random Forest models trained for latitude and longitude.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c812457-52f9-4a33-b803-037d6acb3cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost models trained for latitude and longitude.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Train an XGBoost model for latitude\n",
    "xgb_lat = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_lat.fit(X_train, y_train_lat)\n",
    "\n",
    "# Train an XGBoost model for longitude\n",
    "xgb_lon = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_lon.fit(X_train, y_train_lon)\n",
    "\n",
    "# Print model summary\n",
    "print(\"XGBoost models trained for latitude and longitude.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47186842-4821-4116-a0ad-59fc3c1dc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the AIS test data\n",
    "ais_test = pd.read_csv('data/ais_test.csv', delimiter=',')\n",
    "\n",
    "# Convert time to datetime in test data\n",
    "ais_test['time'] = pd.to_datetime(ais_test['time'])\n",
    "\n",
    "# Initialize empty lists to store the predicted lat/lon for the test set\n",
    "#rf_predicted_latitudes = [np.nan] * len(ais_test)\n",
    "#rf_predicted_longitudes = [np.nan] * len(ais_test)\n",
    "\n",
    "xgb_predicted_latitudes = [np.nan] * len(ais_test)\n",
    "xgb_predicted_longitudes = [np.nan] * len(ais_test)\n",
    "\n",
    "# Iterate over each vessel in the test set\n",
    "for vessel_id in ais_test['vesselId'].unique():\n",
    "    # Extract the historical data for this vessel from the training data\n",
    "    vessel_train_data = ais_train[ais_train['vesselId'] == vessel_id]\n",
    "    \n",
    "    if len(vessel_train_data) > 0:\n",
    "        # Preprocess this vessel's historical data (scale)\n",
    "        vessel_features = scaler.transform(vessel_train_data[features])\n",
    "        \n",
    "        # Predict using Random Forest\n",
    "        #rf_lat_pred = rf_lat.predict(vessel_features)\n",
    "        #rf_lon_pred = rf_lon.predict(vessel_features)\n",
    "        \n",
    "        # Predict using XGBoost\n",
    "        xgb_lat_pred = xgb_lat.predict(vessel_features)\n",
    "        xgb_lon_pred = xgb_lon.predict(vessel_features)\n",
    "        \n",
    "        # Find the indices in ais_test for this vessel\n",
    "        indices = ais_test.index[ais_test['vesselId'] == vessel_id].tolist()\n",
    "        \n",
    "        # Append the last prediction as the next position\n",
    "        for idx in indices:\n",
    "            #rf_predicted_latitudes[idx] = rf_lat_pred[-1]\n",
    "            #rf_predicted_longitudes[idx] = rf_lon_pred[-1]\n",
    "            \n",
    "            xgb_predicted_latitudes[idx] = xgb_lat_pred[-1]\n",
    "            xgb_predicted_longitudes[idx] = xgb_lon_pred[-1]\n",
    "\n",
    "    else:\n",
    "        # If no historical data, append NaN or fallback value\n",
    "        #rf_predicted_latitudes.append(np.nan)\n",
    "        #rf_predicted_longitudes.append(np.nan)\n",
    "        \n",
    "        xgb_predicted_latitudes.append(np.nan)\n",
    "        xgb_predicted_longitudes.append(np.nan)\n",
    "\n",
    "# Store the predictions back into ais_test dataframe\n",
    "#ais_test['latitude_predicted'] = rf_predicted_latitudes\n",
    "#ais_test['longitude_predicted'] = rf_predicted_longitudes\n",
    "\n",
    "ais_test['latitude_predicted'] = xgb_predicted_latitudes\n",
    "ais_test['longitude_predicted'] = xgb_predicted_longitudes\n",
    "\n",
    "# Save predictions to CSV for submission\n",
    "ais_test[['ID', 'longitude_predicted', 'latitude_predicted']].to_csv('predictions_xgb.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba7497dc-01c6-401a-b3cd-90bde4dc732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Select the XGBoost model predictions\n",
    "#ais_test['final_latitude'] = ais_test['xgb_predicted_latitude']\n",
    "#ais_test['final_longitude'] = ais_test['xgb_predicted_longitude']\n",
    "\n",
    "# Option 2: Ensemble by averaging Random Forest and XGBoost predictions\n",
    "#ais_test['latitude_predicted'] = (ais_test['rf_predicted_latitude'] + ais_test['xgb_predicted_latitude']) / 2\n",
    "#ais_test['longitude_predicted'] = (ais_test['rf_predicted_longitude'] + ais_test['xgb_predicted_longitude']) / 2\n",
    "\n",
    "# Save final predictions to CSV\n",
    "#ais_test[['ID', 'longitude_predicted', 'latitude_predicted']].to_csv('final_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a16606f-5dfd-46ed-b263-39ae4a9bfc0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "# Function to calculate weighted geodetic distance\n",
    "def calculate_weighted_distance(predictions, ground_truth):\n",
    "    weights = [0.3, 0.25, 0.2, 0.15, 0.1]\n",
    "    total_distance = 0\n",
    "    total_weight = 0\n",
    "    \n",
    "    for i, weight in enumerate(weights):\n",
    "        pred_lat = predictions[f'final_latitude_day_{i+1}']\n",
    "        pred_lon = predictions[f'final_longitude_day_{i+1}']\n",
    "        true_lat = ground_truth[f'latitude_day_{i+1}']\n",
    "        true_lon = ground_truth[f'longitude_day_{i+1}']\n",
    "        \n",
    "        # Calculate geodetic distance for each day\n",
    "        distance = geodesic((true_lat, true_lon), (pred_lat, pred_lon)).km\n",
    "        total_distance += weight * distance\n",
    "        total_weight += weight\n",
    "    \n",
    "    return total_distance / total_weight  # Weighted average distance\n",
    "\n",
    "# Apply this function to calculate the final evaluation metric\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
